{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from lightfm import LightFM\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использую датасет movielens-1m с первого дз. Датасет популярный, с ним был уже опыт работы, а также -- его используют в статье про NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', delimiter='::', header=None, \n",
    "        names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "        usecols=['user_id', 'movie_id', 'rating'], engine='python')\n",
    "\n",
    "movie_info = pd.read_csv('ml-1m/movies.dat', delimiter='::', header=None, \n",
    "        names=['movie_id', 'name', 'category'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.loc[(ratings['rating'] >= 4)]\n",
    "users = ratings[\"user_id\"]\n",
    "movies = ratings[\"movie_id\"]\n",
    "user_item = sp.coo_matrix((np.ones_like(users), (users, movies)))\n",
    "user_item_csr = user_item.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = np.unique(users)\n",
    "unique_movies = set(np.unique(movies))\n",
    "\n",
    "grouped_interactions = ratings.groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "train_dataset = {}\n",
    "test_dataset = {}\n",
    "negative_dataset = {}\n",
    "\n",
    "for user_id, user_movies in grouped_interactions.iteritems():\n",
    "    if len(user_movies) < 2:\n",
    "        continue\n",
    "\n",
    "    train_dataset[user_id] = user_movies[:-1]\n",
    "    test_dataset[user_id] = user_movies[-1]\n",
    "    negative_dataset[user_id] =  list(unique_movies - set(user_movies))\n",
    "    \n",
    "total_users = list(test_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csr_data(interactions):\n",
    "    coo_users = []\n",
    "    for user_id in interactions:\n",
    "        coo_users.append(np.full(len(interactions[user_id]), user_id))\n",
    "    \n",
    "    coo_users = np.hstack(coo_users)\n",
    "    coo_movies = []\n",
    "    for user_id in interactions:\n",
    "        coo_movies.append(np.array(interactions[user_id]))\n",
    "    \n",
    "    coo_movies = np.hstack(coo_movies)\n",
    "    \n",
    "    user_item = sp.coo_matrix((np.ones_like(coo_users), (coo_users, coo_movies)))\n",
    "    return user_item.tocsr()\n",
    "\n",
    "train_data = extract_csr_data(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично статье из NCF будем сравнивать все модели по метрикам Hit rate(HR@K) и NDCG@K. K = 10\n",
    "Помимо одного позитива, также добавим 99 случайных негативных фильмов для пользователя, тем самым будем оценивать эти метрики относительно ранжирования этих 1 + 99 фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics_for_user(args):\n",
    "    k = 10\n",
    "    model, user_id = args\n",
    "    if user_id not in negative_dataset[user_id]:\n",
    "        return None\n",
    "    \n",
    "    last_user_movie = test_dataset[user_id]\n",
    "    np.random.shuffle(negative_dataset[user_id])\n",
    "    random_negative_movies = negative_dataset[user_id][:99]\n",
    "\n",
    "    input_movies = np.array([last_user_movie] + list(random_negative_movies))\n",
    "    input_user = np.full(len(input_movies), user_id)\n",
    "    \n",
    "    pred = model.predict(input_user, input_movies)\n",
    "\n",
    "    top_movies = input_movies[np.argsort(pred)[-k:]]\n",
    "\n",
    "    hit_rate = 1 if last_user_movie in top_movies else 0\n",
    "\n",
    "    ndcg = 0\n",
    "    for position, movie in enumerate(top_movies):\n",
    "        if movie == last_user_movie:\n",
    "            ndcg = np.log(2) / np.log(position + 2)\n",
    "            break\n",
    "    return hit_rate, ndcg\n",
    "\n",
    "def evaluate_model(model):\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        users_len = len(total_users)\n",
    "        metrics = pool.map(evaluate_metrics_for_user, zip([model] * users_len, total_users))\n",
    "        hrs = [metric[0] for metric in metrics if metric is not None]\n",
    "        ndcgs = [metric[1] for metric in metrics if metric is not None]\n",
    "\n",
    "    print('Mean HR', np.mean(hrs))\n",
    "    print('Mean NDCG', np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично 1 дз будем смотреть на симилары истории игрушек и рекоммендации для 4 пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    \n",
    "    def __init__(self, model, user_emb, item_emb, bias_u=None, bias_i=None):\n",
    "        self.model = model\n",
    "        self.user_emb = user_emb\n",
    "        self.user_bias = bias_u\n",
    "        self.item_emb = item_emb\n",
    "        self.item_bias = bias_i\n",
    "        \n",
    "    def predict(self, users, movies):\n",
    "        return self.model.predict(users, movies)\n",
    "    \n",
    "    def similars(self, toy_movie_id=1, top=10):\n",
    "        input_vector = self.item_emb[toy_movie_id]\n",
    "\n",
    "        data = []\n",
    "        for item_idx, column in enumerate(self.item_emb):\n",
    "            dst = np.linalg.norm(column - input_vector)\n",
    "            data.append((item_idx, dst))\n",
    "\n",
    "        sorted_by_dst = list(sorted(data, key=lambda val: val[1]))\n",
    "\n",
    "        similars = []\n",
    "        for item in sorted_by_dst:\n",
    "            search = movie_info[movie_info[\"movie_id\"] == item[0]]\n",
    "            movie_name = search[\"name\"].to_string()\n",
    "            if len(search) > 0:\n",
    "                similars.append((item[0], movie_name))\n",
    "\n",
    "        return similars[:top]\n",
    "\n",
    "    def recommend(self, user_id=4, top=10):\n",
    "        new_movie_ids = negative_dataset[user_id]\n",
    "\n",
    "        data = []\n",
    "        for movie_id in new_movie_ids:\n",
    "            bias_w = self.user_bias[user_id] if self.user_bias is not None else 0\n",
    "            bias_h = self.item_bias[movie_id] if self.item_bias is not None else 0\n",
    "\n",
    "            dot = np.dot(self.user_emb[user_id], self.item_emb[movie_id])\n",
    "            data.append((movie_id, dot + bias_w + bias_h))\n",
    "\n",
    "        data = list(sorted(data, key=lambda val: val[1], reverse=True))\n",
    "        recommendations = [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() for x in data]\n",
    "        return recommendations[:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline MF model: LightFM warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fe50e238290>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = LightFM(\n",
    "    no_components=64, \n",
    "    learning_rate=0.01,\n",
    "    loss='warp',\n",
    "    max_sampled=200\n",
    ")\n",
    "\n",
    "baseline.fit(train_data, epochs=40, num_threads=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_recommender = Recommender(\n",
    "    baseline, \n",
    "    baseline.user_embeddings, \n",
    "    baseline.item_embeddings,\n",
    "    baseline.user_biases,\n",
    "    baseline.item_biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean HR 0.6832171893147503\n",
      "Mean NDCG 0.2570064577984302\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(baseline_recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '0    Toy Story (1995)'),\n",
       " (588, '584    Aladdin (1992)'),\n",
       " (3114, '3045    Toy Story 2 (1999)'),\n",
       " (2355, \"2286    Bug's Life, A (1998)\"),\n",
       " (1197, '1179    Princess Bride, The (1987)'),\n",
       " (1265, '1245    Groundhog Day (1993)'),\n",
       " (364, '360    Lion King, The (1994)'),\n",
       " (595, '591    Beauty and the Beast (1991)'),\n",
       " (1073, '1058    Willy Wonka and the Chocolate Factory (1971)'),\n",
       " (2321, '2252    Pleasantville (1998)')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_recommender.similars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '847    Godfather, The (1972)',\n",
       " '585    Terminator 2: Judgment Day (1991)',\n",
       " '1192    Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " '1182    Aliens (1986)',\n",
       " '1203    Godfather: Part II, The (1974)',\n",
       " '108    Braveheart (1995)',\n",
       " '2502    Matrix, The (1999)',\n",
       " '453    Fugitive, The (1993)',\n",
       " '537    Blade Runner (1982)']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_recommender.recommend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, total_users, total_items, latent_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP\n",
    "        self.user_embedding = nn.Embedding(total_users, latent_size)\n",
    "        self.item_embedding = nn.Embedding(total_items, latent_size)\n",
    "        \n",
    "        self.mlp_net = nn.Sequential(\n",
    "            nn.Linear(2 * latent_size, latent_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(latent_size, latent_size),\n",
    "        )\n",
    "        \n",
    "        # GMF\n",
    "        self.user_gmf_embedding = nn.Embedding(total_users, latent_size)\n",
    "        self.item_gmf_embedding = nn.Embedding(total_items, latent_size)\n",
    "        \n",
    "        self.rating_layer = nn.Sequential(\n",
    "            nn.Linear(2 * latent_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_user, input_item):\n",
    "        mlp_user_emb = self.user_embedding(input_user)\n",
    "        mlp_item_emb = self.item_embedding(input_item)\n",
    "        mlp_output   = self.mlp_net(torch.cat([mlp_user_emb, mlp_item_emb], dim=-1))\n",
    "        \n",
    "        gmf_user_emb = self.user_gmf_embedding(input_user)\n",
    "        gmf_item_emb = self.item_gmf_embedding(input_item)\n",
    "        gmf_output   = torch.mul(gmf_user_emb, gmf_item_emb)\n",
    "        \n",
    "        ncf_output   = torch.cat([mlp_output, gmf_output], dim=-1)\n",
    "        return self.rating_layer(ncf_output)\n",
    "    \n",
    "    def predict(self, users, movies):\n",
    "        return self(users, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF_Dataset(Dataset):\n",
    "    def __init__(self, train_dataset, negative_dataset):        \n",
    "        coo_users = []\n",
    "        for user_id in train_dataset:\n",
    "            coo_users.append(np.full(len(train_dataset[user_id]), user_id))\n",
    "\n",
    "        coo_users = np.hstack(coo_users)\n",
    "        coo_movies = []\n",
    "        for user_id in train_dataset:\n",
    "            coo_movies.append(np.array(train_dataset[user_id]))\n",
    "\n",
    "        coo_movies = np.hstack(coo_movies)\n",
    "        self.coo_users = coo_users\n",
    "        self.coo_movies = coo_movies\n",
    "        \n",
    "        self.negative_data = negative_dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.coo_users[index]\n",
    "        item_id = self.coo_movies[index]\n",
    "        negative_item_id = random.choice(self.negative_data[user_id])\n",
    "\n",
    "        return user_id, item_id, negative_item_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coo_users)\n",
    "    \n",
    "dataset = NCF_Dataset(train_dataset, negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_embedding): Embedding(6041, 64)\n",
       "  (item_embedding): Embedding(3953, 64)\n",
       "  (mlp_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (user_gmf_embedding): Embedding(6041, 64)\n",
       "  (item_gmf_embedding): Embedding(3953, 64)\n",
       "  (rating_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCF_model = NCF(max(total_users) + 1, max(movies) + 1)\n",
    "device = torch.device(\"cuda\")\n",
    "NCF_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.5533991315596395\n",
      "epoch 10: loss 0.4348882124792758\n",
      "epoch 20: loss 0.3962825050028108\n",
      "epoch 30: loss 0.35017433976955553\n",
      "epoch 40: loss 0.31408113745047894\n",
      "epoch 50: loss 0.2816089452813855\n",
      "epoch 60: loss 0.25491754898278834\n",
      "epoch 70: loss 0.23293632654834995\n",
      "epoch 80: loss 0.2143845393610515\n",
      "epoch 90: loss 0.20199512818948828\n",
      "epoch 100: loss 0.19071783350526\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(NCF_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    trainloader = DataLoader(dataset, batch_size=2048, shuffle=True, num_workers=mp.cpu_count())\n",
    "    for (batch_users, batch_movies, batch_negative_movies) in trainloader:\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_movies = batch_movies.to(device)\n",
    "        batch_negative_movies = batch_negative_movies.to(device)\n",
    "\n",
    "        ncf_positive_output = NCF_model(batch_users, batch_movies)\n",
    "        positive_loss = F.binary_cross_entropy(ncf_positive_output, torch.ones_like(ncf_positive_output).float())\n",
    "        \n",
    "        ncf_negative_output = NCF_model(batch_users, batch_negative_movies)\n",
    "        negative_loss = F.binary_cross_entropy(ncf_negative_output, torch.zeros_like(ncf_negative_output).float())\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch {epoch + 1}: loss {total_loss / total_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_item_emb = NCF_model.item_embedding.weight.detach().cpu().numpy()\n",
    "ncf_user_emb = NCF_model.user_embedding.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_recommender = Recommender(\n",
    "    NCF_model, \n",
    "    ncf_item_emb, \n",
    "    ncf_user_emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_torch_model(model, k=10):\n",
    "    hrs = []\n",
    "    ndcgs = []\n",
    "    for user_id in negative_dataset.keys():\n",
    "        last_user_movie = test_dataset[user_id]\n",
    "        np.random.shuffle(negative_dataset[user_id])\n",
    "        random_negative_movies = negative_dataset[user_id][:99]\n",
    "\n",
    "        input_movies = torch.LongTensor([last_user_movie] + list(random_negative_movies))\n",
    "        input_user = torch.LongTensor(np.full(len(input_movies), user_id))\n",
    "        \n",
    "        pred = model(input_user, input_movies).view(-1).detach().numpy()\n",
    "        top_movies = input_movies[np.argsort(pred)[-k:]]\n",
    "        hit_rate = 1 if last_user_movie in top_movies else 0\n",
    "\n",
    "        ndcg = 0\n",
    "        for position, movie in enumerate(top_movies):\n",
    "            if movie == last_user_movie:\n",
    "                ndcg = np.log(2) / np.log(position + 2)\n",
    "                break\n",
    "        \n",
    "        hrs.append(hit_rate)\n",
    "        ndcgs.append(ndcg)\n",
    "        \n",
    "    print('Mean HR', np.mean(hrs))\n",
    "    print('Mean NDCG', np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean HR 0.67318204406162\n",
      "Mean NDCG 0.24632286384843266\n"
     ]
    }
   ],
   "source": [
    "evaluate_torch_model(NCF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '0    Toy Story (1995)'),\n",
       " (2277, '2208    Somewhere in the City (1997)'),\n",
       " (967, '955    Outlaw, The (1943)'),\n",
       " (213, '211    Burnt By the Sun (Utomlyonnye solntsem) (1994)'),\n",
       " (3565, '3496    Where the Heart Is (2000)'),\n",
       " (1114, '1098    Funeral, The (1996)'),\n",
       " (2289, '2220    Player, The (1992)'),\n",
       " (2170, '2101    Wrongfully Accused (1998)'),\n",
       " (3334, '3265    Key Largo (1948)'),\n",
       " (3621, '3552    Possession (1981)')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf_recommender.similars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_recommend(torch_model, user_id=4, top=10):\n",
    "    new_movie_ids = torch.LongTensor(negative_dataset[user_id])\n",
    "    users = torch.LongTensor(np.full(len(new_movie_ids), user_id))\n",
    "    data = torch_model.predict(users, new_movie_ids).view(-1).detach().numpy()\n",
    "    data = list(zip(new_movie_ids.view(-1).detach().numpy(), data))\n",
    "    data = list(sorted(data, key=lambda val: val[1], reverse=True))\n",
    "    recommendations = [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() for x in data]\n",
    "    return recommendations[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1203    Godfather: Part II, The (1974)',\n",
       " '847    Godfather, The (1972)',\n",
       " '1250    Back to the Future (1985)',\n",
       " '1539    Men in Black (1997)',\n",
       " '1575    L.A. Confidential (1997)',\n",
       " '585    Terminator 2: Judgment Day (1991)',\n",
       " '1284    Butch Cassidy and the Sundance Kid (1969)',\n",
       " '900    Casablanca (1942)',\n",
       " '2693    Sixth Sense, The (1999)',\n",
       " '957    African Queen, The (1951)']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_recommend(NCF_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что рекоммендации ок, т.к. сделаны через предикт, а не через веса эмбеддингов, \n",
    "но с симиларами плохо скорее из-за того, что нужно несколько по-другому считать похожесть, хотя косинусное расстояние работает также плохо.\n",
    "\n",
    "Формально метрики вышли примерно такими же(хуже на 1-2%) как у WARP модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA_Dataset(Dataset):\n",
    "    def __init__(self, train_dataset, negative_dataset, fixed_len=10):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.negative_dataset = negative_dataset\n",
    "        self.fixed_len = fixed_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        user_id_index = index % len(train_dataset.keys())\n",
    "        user_id = list(train_dataset.keys())[user_id_index]\n",
    "        total_history = train_dataset[user_id]\n",
    "        sampled_history = torch.LongTensor(random.choices(total_history, k=self.fixed_len))\n",
    "        rest_targets = list(set(total_history) - set(sampled_history))\n",
    "        target_movie = random.choice(rest_targets)\n",
    "        negaitive_movie = random.choice(self.negative_dataset[user_id])\n",
    "        \n",
    "        return sampled_history, user_id, target_movie, negaitive_movie\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(train_dataset) * 5\n",
    "    \n",
    "sa_dataset = SA_Dataset(train_dataset, negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, total_users, total_items, atten_heads=5, latent_size=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(total_users, latent_size)\n",
    "        self.item_embedding = nn.Embedding(total_items, latent_size)\n",
    "        self.heads = atten_heads\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            latent_size * self.heads, \n",
    "            self.heads, \n",
    "            kdim=latent_size, \n",
    "            vdim=latent_size\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(2 * latent_size + 2 * self.heads, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_history, user, item):\n",
    "        history_embedding = self.item_embedding(user_history)\n",
    "        user_embedding = self.user_embedding(user)\n",
    "        item_embedding = self.item_embedding(item)\n",
    "        \n",
    "        _, attention = self.attention(\n",
    "            item_embedding.repeat(1, 1, self.heads),            \n",
    "            history_embedding, \n",
    "            history_embedding\n",
    "        )\n",
    "        \n",
    "        attention = torch.squeeze(attention)\n",
    "        linear_output = self.linear(torch.cat([user_embedding, item_embedding, attention], dim=-1))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.6935726245244344\n",
      "epoch 10: loss 0.68640216588974\n",
      "epoch 20: loss 0.6628908356030782\n",
      "epoch 30: loss 0.6310411810874939\n",
      "epoch 40: loss 0.5986151019732158\n",
      "epoch 50: loss 0.5667280276616414\n",
      "epoch 60: loss 0.5422204573949178\n",
      "epoch 70: loss 0.5185796181360881\n",
      "epoch 80: loss 0.49953468640645343\n",
      "epoch 90: loss 0.48903797268867494\n",
      "epoch 100: loss 0.4778030614058177\n",
      "epoch 110: loss 0.46613449454307554\n",
      "epoch 120: loss 0.46434036691983543\n",
      "epoch 130: loss 0.45592422485351564\n",
      "epoch 140: loss 0.452245432138443\n",
      "epoch 150: loss 0.451547102133433\n",
      "epoch 160: loss 0.4476594130198161\n",
      "epoch 170: loss 0.4446143269538879\n",
      "epoch 180: loss 0.4465136110782623\n",
      "epoch 190: loss 0.44463562965393066\n",
      "epoch 200: loss 0.4385597387949626\n",
      "epoch 210: loss 0.435043728351593\n",
      "epoch 220: loss 0.4365427533785502\n",
      "epoch 230: loss 0.4395876208941142\n",
      "epoch 240: loss 0.43452355861663816\n",
      "epoch 250: loss 0.4353606879711151\n",
      "epoch 260: loss 0.4355134646097819\n",
      "epoch 270: loss 0.43369463880856834\n",
      "epoch 280: loss 0.43330267469088235\n",
      "epoch 290: loss 0.43410215973854066\n",
      "epoch 300: loss 0.4284358561038971\n",
      "epoch 310: loss 0.4311605791250865\n",
      "epoch 320: loss 0.4348331312338511\n",
      "epoch 330: loss 0.4319723645846049\n",
      "epoch 340: loss 0.42828654845555625\n",
      "epoch 350: loss 0.43269670406977334\n",
      "epoch 360: loss 0.43279816309611\n",
      "epoch 370: loss 0.42711021900177004\n",
      "epoch 380: loss 0.42772141098976135\n",
      "epoch 390: loss 0.4305472950140635\n",
      "epoch 400: loss 0.4292459607124329\n",
      "epoch 410: loss 0.4263651589552561\n",
      "epoch 420: loss 0.4305807848771413\n",
      "epoch 430: loss 0.4309906303882599\n",
      "epoch 440: loss 0.43134307066599525\n",
      "epoch 450: loss 0.4287550429503123\n",
      "epoch 460: loss 0.4308890521526337\n",
      "epoch 470: loss 0.42710282405217487\n",
      "epoch 480: loss 0.4247454543908437\n",
      "epoch 490: loss 0.42918209433555604\n",
      "epoch 500: loss 0.42421167095502216\n"
     ]
    }
   ],
   "source": [
    "sa_model = SelfAttention(max(total_users) + 1, max(movies) + 1)\n",
    "device = torch.device(\"cuda\")\n",
    "sa_model.to(device)\n",
    "optimizer = Adam(sa_model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(500):\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    trainloader = DataLoader(sa_dataset, batch_size=2048, shuffle=True, num_workers=mp.cpu_count())\n",
    "    for (batch_history, batch_users, batch_movies, batch_negative_movies) in trainloader:\n",
    "        batch_history = batch_history.to(device)\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_movies = batch_movies.to(device)\n",
    "        batch_negative_movies = batch_negative_movies.to(device)\n",
    "        sa_positive_output = sa_model(batch_history, batch_users, batch_movies)\n",
    "        positive_loss = F.binary_cross_entropy(sa_positive_output, torch.ones_like(sa_positive_output).float())\n",
    "        \n",
    "        sa_negative_output = sa_model(batch_history, batch_users, batch_negative_movies)\n",
    "        negative_loss = F.binary_cross_entropy(sa_negative_output, torch.zeros_like(sa_negative_output).float())\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch {epoch + 1}: loss {total_loss / total_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_recommender = Recommender(\n",
    "    sa_model.cpu(), \n",
    "    sa_model.user_embedding.weight.detach().cpu().numpy(),\n",
    "    sa_model.item_embedding.weight.detach().cpu().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean HR 0.5234387941030313\n",
      "Mean NDCG 0.20840324458599013\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "hrs = []\n",
    "ndcgs = []\n",
    "for user_id in negative_dataset.keys():\n",
    "    last_user_movie = test_dataset[user_id]\n",
    "    \n",
    "    np.random.shuffle(negative_dataset[user_id])\n",
    "    random_negative_movies = negative_dataset[user_id][:99]\n",
    "    \n",
    "    \n",
    "    input_movies = torch.LongTensor([last_user_movie] + list(random_negative_movies))\n",
    "    input_user = torch.LongTensor(np.full(len(input_movies), user_id))\n",
    "\n",
    "    base_history = train_dataset[user_id][-10:]\n",
    "    if len(base_history) < 10:\n",
    "        base_history = random.choices(base_history, k=10)\n",
    "    user_history = torch.LongTensor(base_history).repeat(len(input_movies), 1)\n",
    "\n",
    "    pred = sa_model(user_history, input_user, input_movies).view(-1).detach().numpy()\n",
    "    top_movies = input_movies[np.argsort(pred)[-k:]]\n",
    "    hit_rate = 1 if last_user_movie in top_movies else 0\n",
    "\n",
    "    ndcg = 0\n",
    "    for position, movie in enumerate(top_movies):\n",
    "        if movie == last_user_movie:\n",
    "            ndcg = np.log(2) / np.log(position + 2)\n",
    "            break\n",
    "\n",
    "    hrs.append(hit_rate)\n",
    "    ndcgs.append(ndcg)\n",
    "\n",
    "print('Mean HR', np.mean(hrs))\n",
    "print('Mean NDCG', np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '0    Toy Story (1995)'),\n",
       " (3015, '2946    Coma (1978)'),\n",
       " (2235, \"2166    One Man's Hero (1999)\"),\n",
       " (1134, '1118    Johnny 100 Pesos (1993)'),\n",
       " (707, '698    Mulholland Falls (1996)'),\n",
       " (2316, '2247    Practical Magic (1998)'),\n",
       " (543, '539    So I Married an Axe Murderer (1993)'),\n",
       " (2792, '2723    Airplane II: The Sequel (1982)'),\n",
       " (1727, '1678    Horse Whisperer, The (1998)'),\n",
       " (114, \"112    Margaret's Museum (1995)\")]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_recommender.similars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2789    American Beauty (1999)',\n",
       " '1192    Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " '589    Silence of the Lambs, The (1991)',\n",
       " '2693    Sixth Sense, The (1999)',\n",
       " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '1250    Back to the Future (1985)',\n",
       " '585    Terminator 2: Judgment Day (1991)',\n",
       " '2327    Shakespeare in Love (1998)',\n",
       " \"523    Schindler's List (1993)\",\n",
       " '847    Godfather, The (1972)']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sa_recommend(sa_model, user_id=4, top=10):\n",
    "    new_movie_ids = torch.LongTensor(negative_dataset[user_id])\n",
    "    users = torch.LongTensor(np.full(len(new_movie_ids), user_id))\n",
    "    user_history = torch.LongTensor(random.choices(train_dataset[user_id], k=10)).repeat(len(new_movie_ids), 1)\n",
    "    data = sa_model(user_history, users, new_movie_ids).view(-1).detach().numpy()\n",
    "    data = list(zip(new_movie_ids.view(-1).detach().numpy(), data))\n",
    "    data = list(sorted(data, key=lambda val: val[1], reverse=True))\n",
    "    recommendations = [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() for x in data]\n",
    "    return recommendations[:top]\n",
    "\n",
    "sa_recommend(sa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь аналогично с симиларами и рекомендациями но с метриками уже похуже."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
